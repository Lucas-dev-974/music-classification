{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-08 23:50:12.571875: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-08-08 23:50:12.617737: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-08 23:50:12.949610: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-08 23:50:12.951319: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-08 23:50:14.061227: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/lucas/Work/Simplon/music-classification/api/.venv/lib/python3.11/site-packages/lazy_loader/__init__.py:185: RuntimeWarning: subpackages can technically be lazily loaded, but it causes the package to be eagerly loaded even if it is already lazily loaded.So, you probably shouldn't use subpackages with this lazy feature.\n",
      "  warnings.warn(msg, RuntimeWarning)\n",
      "/home/lucas/Work/Simplon/music-classification/api/.venv/lib/python3.11/site-packages/lazy_loader/__init__.py:185: RuntimeWarning: subpackages can technically be lazily loaded, but it causes the package to be eagerly loaded even if it is already lazily loaded.So, you probably shouldn't use subpackages with this lazy feature.\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import librosa # Pour l'extraction des features et la lecture des fichiers wav\n",
    "import librosa.display # Pour récupérer les spectrogrammes des audio\n",
    "import librosa.feature\n",
    "import os \n",
    "\n",
    "def model():\n",
    "    model = tf.keras.models.Sequential()\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(32, kernel_size=(3, 3),\n",
    "                    activation='relu',\n",
    "                    input_shape=(128,660,1)))\n",
    "\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(64, \n",
    "                    kernel_size=(3, 3), \n",
    "                    activation='relu'))\n",
    "\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "\n",
    "    model.add(tf.keras.layers.Conv2D(32, \n",
    "                    kernel_size=(3, 3), \n",
    "                    activation='relu'))\n",
    "\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dropout(0.3))\n",
    "    model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "\n",
    "    model.add(tf.keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "genres = ['blues', 'classical', 'country', 'disco', 'hiphop', \n",
    "          'jazz', 'metal', 'pop', 'reggae', 'rock']\n",
    "\n",
    "audio_files = {}\n",
    "\n",
    "for g in genres:\n",
    "  audio_files[g] = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "for g in genres:\n",
    "  for audio in os.listdir(f'./dataset/genres_original/{g}'):\n",
    "    audio_files[g].append(librosa.core.load(f'./dataset/genres_original/{g}/{audio}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ParameterError",
     "evalue": "Audio data must be of type numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mParameterError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 34\u001b[0m\n\u001b[1;32m     31\u001b[0m     y \u001b[39m=\u001b[39m audio[\u001b[39m0\u001b[39m]\n\u001b[1;32m     32\u001b[0m     sr \u001b[39m=\u001b[39m audio[\u001b[39m1\u001b[39m]\n\u001b[0;32m---> 34\u001b[0m     spect \u001b[39m=\u001b[39m librosa\u001b[39m.\u001b[39;49mfeature\u001b[39m.\u001b[39;49mmelspectrogram(y\u001b[39m=\u001b[39;49my, sr\u001b[39m=\u001b[39;49msr, n_fft\u001b[39m=\u001b[39;49m\u001b[39m2048\u001b[39;49m, hop_length\u001b[39m=\u001b[39;49m\u001b[39m1024\u001b[39;49m)\n\u001b[1;32m     35\u001b[0m     spect \u001b[39m=\u001b[39m librosa\u001b[39m.\u001b[39mpower_to_db(spect, ref\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mmax)\n\u001b[1;32m     37\u001b[0m \u001b[39m# On modifie la taille des images 128 x 660 en gardant les paramètres proposés dans l'article initial\u001b[39;00m\n",
      "File \u001b[0;32m~/Work/Simplon/music-classification/api/.venv/lib/python3.11/site-packages/librosa/feature/spectral.py:2144\u001b[0m, in \u001b[0;36mmelspectrogram\u001b[0;34m(y, sr, S, n_fft, hop_length, win_length, window, center, pad_mode, power, **kwargs)\u001b[0m\n\u001b[1;32m   2021\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmelspectrogram\u001b[39m(\n\u001b[1;32m   2022\u001b[0m     \u001b[39m*\u001b[39m,\n\u001b[1;32m   2023\u001b[0m     y: Optional[np\u001b[39m.\u001b[39mndarray] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2033\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any,\n\u001b[1;32m   2034\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m np\u001b[39m.\u001b[39mndarray:\n\u001b[1;32m   2035\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Compute a mel-scaled spectrogram.\u001b[39;00m\n\u001b[1;32m   2036\u001b[0m \n\u001b[1;32m   2037\u001b[0m \u001b[39m    If a spectrogram input ``S`` is provided, then it is mapped directly onto\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2141\u001b[0m \u001b[39m    >>> ax.set(title='Mel-frequency spectrogram')\u001b[39;00m\n\u001b[1;32m   2142\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 2144\u001b[0m     S, n_fft \u001b[39m=\u001b[39m _spectrogram(\n\u001b[1;32m   2145\u001b[0m         y\u001b[39m=\u001b[39;49my,\n\u001b[1;32m   2146\u001b[0m         S\u001b[39m=\u001b[39;49mS,\n\u001b[1;32m   2147\u001b[0m         n_fft\u001b[39m=\u001b[39;49mn_fft,\n\u001b[1;32m   2148\u001b[0m         hop_length\u001b[39m=\u001b[39;49mhop_length,\n\u001b[1;32m   2149\u001b[0m         power\u001b[39m=\u001b[39;49mpower,\n\u001b[1;32m   2150\u001b[0m         win_length\u001b[39m=\u001b[39;49mwin_length,\n\u001b[1;32m   2151\u001b[0m         window\u001b[39m=\u001b[39;49mwindow,\n\u001b[1;32m   2152\u001b[0m         center\u001b[39m=\u001b[39;49mcenter,\n\u001b[1;32m   2153\u001b[0m         pad_mode\u001b[39m=\u001b[39;49mpad_mode,\n\u001b[1;32m   2154\u001b[0m     )\n\u001b[1;32m   2156\u001b[0m     \u001b[39m# Build a Mel filter\u001b[39;00m\n\u001b[1;32m   2157\u001b[0m     mel_basis \u001b[39m=\u001b[39m filters\u001b[39m.\u001b[39mmel(sr\u001b[39m=\u001b[39msr, n_fft\u001b[39m=\u001b[39mn_fft, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[0;32m~/Work/Simplon/music-classification/api/.venv/lib/python3.11/site-packages/librosa/core/spectrum.py:2838\u001b[0m, in \u001b[0;36m_spectrogram\u001b[0;34m(y, S, n_fft, hop_length, power, win_length, window, center, pad_mode)\u001b[0m\n\u001b[1;32m   2832\u001b[0m     \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   2833\u001b[0m         \u001b[39mraise\u001b[39;00m ParameterError(\n\u001b[1;32m   2834\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mInput signal must be provided to compute a spectrogram\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   2835\u001b[0m         )\n\u001b[1;32m   2836\u001b[0m     S \u001b[39m=\u001b[39m (\n\u001b[1;32m   2837\u001b[0m         np\u001b[39m.\u001b[39mabs(\n\u001b[0;32m-> 2838\u001b[0m             stft(\n\u001b[1;32m   2839\u001b[0m                 y,\n\u001b[1;32m   2840\u001b[0m                 n_fft\u001b[39m=\u001b[39;49mn_fft,\n\u001b[1;32m   2841\u001b[0m                 hop_length\u001b[39m=\u001b[39;49mhop_length,\n\u001b[1;32m   2842\u001b[0m                 win_length\u001b[39m=\u001b[39;49mwin_length,\n\u001b[1;32m   2843\u001b[0m                 center\u001b[39m=\u001b[39;49mcenter,\n\u001b[1;32m   2844\u001b[0m                 window\u001b[39m=\u001b[39;49mwindow,\n\u001b[1;32m   2845\u001b[0m                 pad_mode\u001b[39m=\u001b[39;49mpad_mode,\n\u001b[1;32m   2846\u001b[0m             )\n\u001b[1;32m   2847\u001b[0m         )\n\u001b[1;32m   2848\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m power\n\u001b[1;32m   2849\u001b[0m     )\n\u001b[1;32m   2851\u001b[0m \u001b[39mreturn\u001b[39;00m S, n_fft\n",
      "File \u001b[0;32m~/Work/Simplon/music-classification/api/.venv/lib/python3.11/site-packages/librosa/core/spectrum.py:229\u001b[0m, in \u001b[0;36mstft\u001b[0;34m(y, n_fft, hop_length, win_length, window, center, dtype, pad_mode, out)\u001b[0m\n\u001b[1;32m    226\u001b[0m     \u001b[39mraise\u001b[39;00m ParameterError(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mhop_length=\u001b[39m\u001b[39m{\u001b[39;00mhop_length\u001b[39m}\u001b[39;00m\u001b[39m must be a positive integer\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    228\u001b[0m \u001b[39m# Check audio is valid\u001b[39;00m\n\u001b[0;32m--> 229\u001b[0m util\u001b[39m.\u001b[39;49mvalid_audio(y, mono\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[1;32m    231\u001b[0m fft_window \u001b[39m=\u001b[39m get_window(window, win_length, fftbins\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m    233\u001b[0m \u001b[39m# Pad the window out to n_fft size\u001b[39;00m\n",
      "File \u001b[0;32m~/Work/Simplon/music-classification/api/.venv/lib/python3.11/site-packages/librosa/util/utils.py:297\u001b[0m, in \u001b[0;36mvalid_audio\u001b[0;34m(y, mono)\u001b[0m\n\u001b[1;32m    241\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Determine whether a variable contains valid audio data.\u001b[39;00m\n\u001b[1;32m    242\u001b[0m \n\u001b[1;32m    243\u001b[0m \u001b[39mThe following conditions must be satisfied:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    293\u001b[0m \u001b[39mnumpy.float32\u001b[39;00m\n\u001b[1;32m    294\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(y, np\u001b[39m.\u001b[39mndarray):\n\u001b[0;32m--> 297\u001b[0m     \u001b[39mraise\u001b[39;00m ParameterError(\u001b[39m\"\u001b[39m\u001b[39mAudio data must be of type numpy.ndarray\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    299\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m np\u001b[39m.\u001b[39missubdtype(y\u001b[39m.\u001b[39mdtype, np\u001b[39m.\u001b[39mfloating):\n\u001b[1;32m    300\u001b[0m     \u001b[39mraise\u001b[39;00m ParameterError(\u001b[39m\"\u001b[39m\u001b[39mAudio data must be floating-point\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mParameterError\u001b[0m: Audio data must be of type numpy.ndarray"
     ]
    }
   ],
   "source": [
    "\n",
    "# Remplissage du dictionnaire en important les fichiers audio avec Librosa :\n",
    "for g in genres:\n",
    "  for audio in os.listdir(f'./dataset/genres_original/{g}'):\n",
    "    audio_files[g].append(librosa.load(f'./dataset/genres_original/{g}/{audio}')[0])\n",
    "    \n",
    "for g in genres:\n",
    "  for audio in audio_files[g]:\n",
    "\n",
    "    y = audio[0]\n",
    "    sr = audio[1]\n",
    "\n",
    "    spect = librosa.feature.melspectrogram(y=y, sr=sr, n_fft=2048, hop_length=1024)\n",
    "    spect = librosa.power_to_db(spect, ref=np.max)\n",
    "        \n",
    "# On modifie la taille des images 128 x 660 en gardant les paramètres proposés dans l'article initial\n",
    "    if spect.shape[1] != 660:\n",
    "      spect.resize(128,660, refcheck=False)\n",
    "\n",
    "    mel_specs.append(spect)\n",
    "        \n",
    "X = np.array(mel_specs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
